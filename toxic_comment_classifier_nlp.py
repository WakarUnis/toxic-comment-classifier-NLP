# -*- coding: utf-8 -*-
"""toxic comment classifier NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PSTqG5NXVxXaIRvDNYnjFcctp44pS5nR

# **Toxic Comment Classification using NLP**

*Importing libraries*
"""

!pip install nlp_utils
!pip install contractions

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
import re
import nltk
import string
import nlp_utils
import contractions
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer

"""*Importing Dataset*"""

dataset = pd.read_csv('/content/traindata.csv')
dataset

dataset.info()

"""*Checking the null values*"""

dataset.isnull().sum()

"""*Counting threating sentence*"""

dataset['toxic'].value_counts()

dataset['severe_toxic'].value_counts()

dataset['obscene'].value_counts()

dataset['threat'].value_counts()

dataset['insult'].value_counts()

dataset['identity_hate'].value_counts()

"""# ***Data Visualization***"""

#taking only numerical columns
num_col = dataset.iloc[:,2:].sum()
num_col

sns.set_style("darkgrid")
nc = num_col.sort_values(ascending=False)
plt.figure(figsize=(15,8))
temp =sns.barplot(nc.index, nc.values, alpha=0.8) 
plt.title('SentenceType')
plt.ylabel('COUNT', fontsize=14)
plt.xlabel('All Sentence Types', fontsize=15)
temp.set_xticklabels(rotation=90,labels=nc.index,fontsize=10)
plt.show()

"""# ***Pre-processing the data***

*Removing spacial characters from comments*
"""

import re
import string
alphanumeric = lambda x: re.sub('\w*\d\w*', ' ', x)
punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())
remove_n = lambda x: re.sub("\n", " ", x)
remove_non_ascii = lambda x: re.sub(r'[^\x00-\x7f]',r' ', x)
dataset['comment_text'] = dataset['comment_text'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)

"""*Creating different dataframe*"""

insulting_comment = dataset.loc[:,['id','comment_text','insult']]
hate_comment = dataset.loc[:,['id','comment_text','identity_hate']]
threatening_comment = dataset.loc[:,['id','comment_text','threat']]
obscene_comment = dataset.loc[:,['id','comment_text','obscene']]
severe_toxic_comment = dataset.loc[:,['id','comment_text','severe_toxic']]
toxic_comment = dataset.loc[:,['id','comment_text','toxic']]

"""# ***Visualizing subset datasets using wordcloud***"""

import wordcloud
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from nltk.corpus import stopwords
def wordcloud(dataset, label):
    
    subset=dataset[dataset[label]==1]
    text=subset.comment_text.values
    wc= WordCloud(background_color="black",max_words=2000)
    wc.generate(" ".join(text))
    plt.figure(figsize=(20,20))
    plt.subplot(221)
    plt.axis("off")
    plt.title("Words frequented in {}".format(label), fontsize=15)
    plt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=0), alpha=0.98)

wordcloud(severe_toxic_comment,'severe_toxic')

wordcloud(obscene_comment,'obscene')

wordcloud(toxic_comment,'toxic')

wordcloud(threatening_comment,'threat')

wordcloud(insulting_comment,'insult')

wordcloud(hate_comment,'identity_hate')

"""# ***Balancing the target column in the dataset***"""

toxic_comment['toxic'].value_counts()

balanced_toxic_comment_1 = toxic_comment[toxic_comment['toxic'] == 1].iloc[0:15294,:]
balanced_toxic_comment_0 = toxic_comment[toxic_comment['toxic'] == 0].iloc[0:15294,:]
balanced_toxic_comment = pd.concat([balanced_toxic_comment_1, balanced_toxic_comment_0])
balanced_toxic_comment['toxic'].value_counts()

severe_toxic_comment['severe_toxic'].value_counts()

balanced_severe_toxic_comment_1 = severe_toxic_comment[severe_toxic_comment['severe_toxic'] == 1].iloc[0:1595,:]
balanced_severe_toxic_comment_0 = severe_toxic_comment[severe_toxic_comment['severe_toxic'] == 0].iloc[0:1595,:]
balanced_severe_toxic_comment = pd.concat([balanced_severe_toxic_comment_1, balanced_severe_toxic_comment_0])
balanced_severe_toxic_comment['severe_toxic'].value_counts()

obscene_comment['obscene'].value_counts()

balanced_obscene_comment_1 = obscene_comment[obscene_comment['obscene'] == 1].iloc[0:8449,:] 
balanced_obscene_comment_0 = obscene_comment[obscene_comment['obscene'] == 0].iloc[0:8449,:]
balanced_obscene_comment = pd.concat([balanced_obscene_comment_1,balanced_obscene_comment_0])
balanced_obscene_comment['obscene'].value_counts()

threatening_comment['threat'].value_counts()

balanced_threatening_comment_1 = threatening_comment[threatening_comment['threat'] == 1].iloc[0:478,:]
balanced_threatening_comment_0 = threatening_comment[threatening_comment['threat'] == 0].iloc[0:478,:]
balanced_threatening_comment = pd.concat([balanced_threatening_comment_1,balanced_threatening_comment_0])
balanced_threatening_comment['threat'].value_counts()

insulting_comment['insult'].value_counts()

balanced_insulting_comment_1 = insulting_comment[insulting_comment['insult'] == 1].iloc[0:7877,:]
balanced_insulting_comment_0 = insulting_comment[insulting_comment['insult'] == 0].iloc[0:7877,:]
balanced_insulting_comment = pd.concat([balanced_insulting_comment_1,balanced_insulting_comment_0])
balanced_insulting_comment['insult'].value_counts()

hate_comment['identity_hate'].value_counts()

balanced_hate_comment_1 = hate_comment[hate_comment['identity_hate'] == 1].iloc[0:1405,:]
balanced_hate_comment_0 = hate_comment[hate_comment['identity_hate'] == 0].iloc[0:1405,:]
balanced_hate_comment = pd.concat([balanced_hate_comment_1, balanced_hate_comment_0])
balanced_hate_comment['identity_hate'].value_counts()

"""# ***Machine learning***"""

from sklearn import preprocessing
from sklearn.feature_selection import SelectFromModel

from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_curve, fbeta_score, confusion_matrix
from sklearn.metrics import roc_auc_score, roc_curve

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from nltk import ngrams,bigrams,trigrams

"""# *Splitting the dataset and apply different regression model*"""

def cv_tf_train_test(dataframe,label,vectorizer,ngram):

    # Split the data into X and y data sets
    X = dataframe.comment_text
    y = dataframe[label]

    # Split our data into training and test data 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)

    # Using vectorizer and removing stopwords
    cv1 = vectorizer(ngram_range=(ngram), stop_words='english')
    
    # Transforming x-train and x-test
    X_train_cv1 = cv1.fit_transform(X_train) 
    X_test_cv1  = cv1.transform(X_test)      
    
    ## Machine learning models   
    
    ## Logistic regression
    lr = LogisticRegression()
    lr.fit(X_train_cv1, y_train)
    
    ## k-nearest neighbours
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train_cv1, y_train)

    ## Naive Bayes
    bnb = BernoulliNB()
    bnb.fit(X_train_cv1, y_train)
    
    ## Multinomial naive bayes
    mnb = MultinomialNB()
    mnb.fit(X_train_cv1, y_train)
     ## Support vector machine
    svm_model = LinearSVC()
    svm_model.fit(X_train_cv1, y_train)

    ## Random Forest 
    randomforest = RandomForestClassifier(n_estimators=100, random_state=50)
    randomforest.fit(X_train_cv1, y_train)
    
    f1_score_data = {'F1 Score':[f1_score(lr.predict(X_test_cv1), y_test), f1_score(knn.predict(X_test_cv1), y_test), 
                                f1_score(bnb.predict(X_test_cv1), y_test), f1_score(mnb.predict(X_test_cv1), y_test),
                                f1_score(svm_model.predict(X_test_cv1), y_test), f1_score(randomforest.predict(X_test_cv1), y_test)]} 
    ## Saving f1 score results into a dataframe                     
    dataset_f1 = pd.DataFrame(f1_score_data, index=['Log Regression','KNN', 'BernoulliNB', 'MultinomialNB', 'SVM', 'Random Forest'])  

    return dataset_f1

"""***Evaluating model performance***"""

severe_toxic_comment_cv = cv_tf_train_test(balanced_severe_toxic_comment, 'severe_toxic', TfidfVectorizer, (1,1))
severe_toxic_comment_cv.rename(columns={'F1 Score': 'F1 Score(severe_toxic)'}, inplace=True)
severe_toxic_comment_cv

obscene_comment_cv = cv_tf_train_test(balanced_obscene_comment, 'obscene', TfidfVectorizer, (1,1))
obscene_comment_cv.rename(columns={'F1 Score': 'F1 Score(obscene)'}, inplace=True)
obscene_comment_cv

threat_comment_cv = cv_tf_train_test(balanced_threatening_comment, 'threat', TfidfVectorizer, (1,1))
threat_comment_cv.rename(columns={'F1 Score': 'F1 Score(threat)'}, inplace=True)
threat_comment_cv

insult_comment_cv = cv_tf_train_test(balanced_insulting_comment, 'insult', TfidfVectorizer, (1,1))
insult_comment_cv.rename(columns={'F1 Score': 'F1 Score(insult)'}, inplace=True)
insult_comment_cv

identity_hatecomment_cv = cv_tf_train_test(balanced_hate_comment, 'identity_hate', TfidfVectorizer, (1,1))
identity_hatecomment_cv.rename(columns={'F1 Score': 'F1 Score(identity_hate)'}, inplace=True)
identity_hatecomment_cv

"""# *Testing the model to check if the given text is toxic or not.*"""

X = balanced_toxic_comment.comment_text
y = balanced_toxic_comment['toxic']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initiate a Tfidf vectorizer
tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')

X_train_fit = tfv.fit_transform(X_train)  
X_test_fit = tfv.transform(X_test)  
randomforest = RandomForestClassifier(n_estimators=100, random_state=50)

randomforest.fit(X_train_fit, y_train)
randomforest.predict(X_test_fit)

comment1 = ['i ate an insect']
comment1_vect = tfv.transform(comment1)
randomforest.predict_proba(comment1_vect)[:,1]

comment2 = ['Is this sentence a good one']
comment2_vect = tfv.transform(comment2)
randomforest.predict_proba(comment2_vect)[:,1]